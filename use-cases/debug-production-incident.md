---
title: "Debug Production Incidents with AI Log Analysis"
slug: debug-production-incident
description: "Quickly identify root causes in production incidents by analyzing logs, correlating errors, and generating incident reports."
skills: [log-analyzer, data-analysis]
category: devops
tags: [debugging, incident-response, logs, production, observability]
---

# Debug Production Incidents with AI Log Analysis

## The Problem

Production incidents hit at the worst times. Engineers scramble through thousands of log lines across multiple services, grepping for patterns while Slack channels explode with "is the site down?" messages. Finding the root cause in a sea of cascading failures takes 30-60 minutes even for experienced engineers — and that's before writing the incident report.

## The Solution

Use the **log-analyzer** skill to have your AI agent parse logs, build a timeline, and pinpoint the root cause. Combine with **data-analysis** to correlate metrics and error rates. The agent reads your logs, identifies the first failure, traces the cascade, and produces a structured incident report.

```bash
npx terminal-skills install log-analyzer
npx terminal-skills install data-analysis
```

## Step-by-Step Walkthrough

### 1. Feed the agent your logs

```
Analyze these application logs from the last 30 minutes. We're getting reports of failed checkouts. The logs are in /var/log/app/checkout-service.log
```

### 2. The agent identifies the error pattern

It scans the logs chronologically, filters errors and warnings, and groups them by type:

```
Found 3 distinct error types in the last 30 minutes:

| Error Type                  | Count | First Seen | Last Seen |
|-----------------------------|-------|------------|-----------|
| Redis ConnectionTimeout     | 2,341 | 14:02:03   | 14:31:45  |
| CartService NullPointer     | 412   | 14:02:08   | 14:31:40  |
| PaymentGateway 503          | 189   | 14:05:22   | 14:31:42  |

First error: Redis ConnectionTimeout at 14:02:03
```

### 3. Ask for cross-service correlation

```
Now check the Redis server logs at /var/log/redis/redis-server.log for the same time window.
```

The agent finds that Redis hit max memory at 14:01:58, five seconds before the checkout errors started:

```
14:01:58.332 # WARNING: maxmemory reached, eviction policy: noeviction
14:01:58.333 # Client connection rejected: OOM command not allowed
```

### 4. Generate the incident timeline

```
Build an incident timeline and recommend immediate fixes.
```

```
## Incident Timeline
- 14:01:58 — Redis hit maxmemory limit (16GB), started rejecting connections
- 14:02:03 — Checkout service: first ConnectionTimeout to Redis
- 14:02:08 — Cart lookups fail → NullPointerException in CartService
- 14:05:22 — Payment requests fail due to missing cart data → 503s
- 14:12:00 — Alert fired in PagerDuty

## Root Cause
Redis instance reached its 16GB memory limit with noeviction policy,
causing all new write/read connections to be rejected. This cascaded
into checkout and payment failures.

## Immediate Actions
1. Increase Redis maxmemory to 24GB or switch to allkeys-lru eviction
2. Flush expired session keys: redis-cli --scan --pattern "sess:*"
3. Restart checkout-service pods to clear connection pool errors
```

### 5. Analyze error rate trends

```
Parse the error counts per minute from the logs and show me the trend.
```

The agent produces a distribution showing the sharp spike at 14:02 and whether errors are still occurring or have stabilized.

## Real-World Example

Marta, a platform engineer at a 20-person fintech startup, gets paged at 2 AM because checkout success rates dropped from 99.2% to 34%. She opens her terminal and pastes the last 30 minutes of logs from three services.

1. She asks the agent: "Analyze these logs and find why checkouts are failing"
2. The agent traces the cascade: Redis OOM → cart lookup failures → payment timeouts
3. She asks: "What's the fastest fix right now?"
4. The agent suggests increasing Redis maxmemory and flushing stale session keys
5. She applies the fix, asks the agent to monitor the next 5 minutes of logs
6. Error rate drops back to baseline — incident resolved in 12 minutes instead of the usual 45

The next morning, she asks the agent to generate a formal incident report from the timeline for the post-mortem.

## Related Skills

- [docker-helper](../skills/docker-helper/) — Check container resource limits and restart crashed services
- [data-analysis](../skills/data-analysis/) — Chart error rates and latency during incidents
- [cicd-pipeline](../skills/cicd-pipeline/) — Roll back a bad deployment that caused the incident

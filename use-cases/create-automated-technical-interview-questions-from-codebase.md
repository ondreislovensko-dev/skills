---
title: "Create Automated Technical Interview Questions from Codebase with AI"
slug: create-automated-technical-interview-questions-from-codebase
description: "Generate relevant technical interview questions based on your actual codebase so candidates solve problems they'd face on the job."
skills: [coding-agent, applicant-screening, markdown-writer]
category: business
tags: [hiring, interview, technical-assessment, codebase-analysis]
---

# Create Automated Technical Interview Questions from Codebase with AI

## The Problem

A 25-person startup is hiring three backend engineers but their interview process uses generic LeetCode-style questions that don't reflect the actual work. Candidates who ace algorithm puzzles sometimes struggle with the real codebase — event-driven architecture, complex database queries, and third-party API integrations. Meanwhile, strong practical engineers who would thrive on the team fail the whiteboard rounds. The engineering manager spends 5 hours per role crafting take-home assignments from scratch, and they go stale as the codebase evolves. There's no systematic way to generate interview questions that match what the team actually builds.

## The Solution

Use the **coding-agent** to analyze your codebase and identify representative patterns, **applicant-screening** to structure the assessment criteria, and **markdown-writer** to produce polished interview question documents with rubrics.

```bash
npx terminal-skills install coding-agent applicant-screening markdown-writer
```

## Step-by-Step Walkthrough

### 1. Analyze the codebase for interview-worthy patterns

Tell the agent what role you're hiring for:

```
Analyze our codebase and generate technical interview questions for a mid-level backend engineer. Focus on patterns they'd actually encounter: our event-driven architecture, database query patterns, API design conventions, and error handling approach. Generate questions at easy, medium, and hard difficulty levels.
```

### 2. The agent identifies key patterns

```
Codebase analysis complete — patterns identified:

Architecture patterns:
- Event-driven pub/sub with RabbitMQ (42 event handlers)
- Repository pattern for database access (18 repositories)
- Middleware-based request validation (23 validators)

Common tasks (by commit frequency):
- Adding new API endpoints with validation
- Writing event handlers with retry logic
- Complex SQL queries with joins across 3+ tables
- Third-party webhook processing with idempotency

Error handling conventions:
- Custom error classes with error codes
- Circuit breaker pattern for external APIs
- Structured logging with correlation IDs
```

### 3. Review generated questions

The agent produces tiered questions based on real code patterns:

```
Generated 9 interview questions:

Easy (warm-up, 15 min each):
1. "Given this API endpoint skeleton, add input validation for the user registration route using our middleware pattern"
2. "This database query returns duplicate rows. Find and fix the join issue"
3. "Write a unit test for this event handler, mocking the message queue"

Medium (core assessment, 30 min each):
4. "Implement a new webhook receiver that handles Stripe payment events with idempotency"
5. "Refactor this N+1 query into an efficient batch query using our repository pattern"
6. "Add circuit breaker logic to this third-party API client that currently fails silently"

Hard (senior signal, 45 min each):
7. "Design an event-driven saga for our order fulfillment flow, handling partial failures"
8. "This event handler processes messages out of order under load. Implement ordering guarantees"
9. "Optimize this report query that times out on tables with 10M+ rows"
```

### 4. Generate rubrics and evaluation guides

```
Create scoring rubrics for each question. Include what a strong answer looks like, common mistakes, and follow-up questions to probe deeper.
```

```
Rubrics generated: interview-questions/rubrics.md

For each question:
- Expected approach (what a strong candidate does first)
- Key signals to watch for (error handling, edge cases, testing)
- Red flags (ignoring validation, no error handling, hardcoded values)
- Follow-up questions (2-3 probing questions per problem)
- Time expectations (how long each difficulty level should take)
```

### 5. Create a take-home option

```
Create a simplified take-home assignment based on question #4. Strip proprietary code, add a README with setup instructions, and include an automated test suite that validates the submission.
```

The agent packages a self-contained take-home repo with the problem, boilerplate, and test harness.

## Real-World Example

Aisha manages engineering at a 25-person fintech startup hiring three backend engineers. She's tired of candidates who ace LeetCode but can't work with event-driven systems.

1. She asks the agent to analyze the codebase and generate interview questions for mid-level backend engineers
2. The agent identifies the team's core patterns — event-driven messaging, repository pattern, and webhook processing — and generates 9 tiered questions based on real code
3. Each question comes with a rubric so any interviewer on the team can evaluate consistently
4. She converts one medium question into a take-home assignment with automated tests
5. In the next hiring round, two candidates who would have failed LeetCode-style interviews excel on the practical questions and get hired. Both are productive within their first week because the problems mirrored actual work

### Tips for Better Results

- Regenerate questions quarterly as the codebase evolves — stale questions test patterns the team no longer uses
- Include context from real pull requests so questions reflect actual code review scenarios
- Calibrate difficulty by having existing team members try the questions first
- Create multiple variants of each question to prevent candidates from sharing answers
- Pair take-home questions with a live discussion to assess the candidate's reasoning, not just their solution
- Strip all proprietary business logic — use the patterns but anonymize the domain

## Related Skills

- [coding-agent](../skills/coding-agent/) -- Analyzes codebase patterns and generates representative code problems
- [applicant-screening](../skills/applicant-screening/) -- Structures assessment criteria and evaluation frameworks
- [markdown-writer](../skills/markdown-writer/) -- Produces polished question documents and scoring rubrics
